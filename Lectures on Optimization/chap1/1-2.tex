\section{Local Methods in Unconstrained Minimization}\label{sec:local_methods_in_unconstrained_min}

In this section, we consider several methods for solving the following unconstrained minimization problem:
\begin{equation}\label{eq:unconstrained_nonlinear_problem}
    \min_{\bm{x} \in \R^n} f(\bm{x}),
\end{equation}
where \(f(\cdot)\) is a smooth function.

\subsection{Relaxation and Approximation}\label{subsec:relax_approx}

The simplest goal in general Nonlinear Optimization consists in \emph{finding a local minimum of a differentiable function}. 
The majority of methods in general Nonlinear Optimization are based on the idea of \emph{relaxation}.

\begin{defn}[Relaxation]\label{defn:relaxation}
    A sequence of real numbers \(\{ a_k \}_{k=0}^\infty\) is called a \emph{relaxation sequence} if 
    \[
        a_{k+1} \le a_k \quad \forall k > 0. 
    \]
\end{defn}

In order to deal with problem~\ref{eq:unconstrained_nonlinear_problem}, most of methods generate a relaxation sequence of function values \(\{ f(\bm{x}_k) \}_{k=0}^{\infty}\):
\[
    f(\bm{x}_{k+1}) \le f(\bm{x}_k), \quad k = 0, 1, \dots ~.  
\]
This rule has the following important advantages:
\begin{enumerate}
    \item If \(f(\cdot)\) is bounded below on \(\R^n\), then the sequence \(\{ f(\bm{x}_k) \}_{k=0}^{\infty}\) converges.
    \item In any case, we improve the initial value of the objective function.
\end{enumerate}

In the meanwhile, there is another concept which is essential to implement the idea of relaxation. That is \emph{approximation}.
\begin{defn}[Approximation]\label{defn:approx}
    To approximate means to replace an initial complex object by a simper one which is close to the original in terms of its properties.
\end{defn}

In Nonlinear Optimization, we usually apply \emph{local approximation} based on derivatives of nonlinear functions. These are \(1\)-st- and \(2\)-nd-order approximations (or, the linear and quadratic approximations).

\begin{defn}[Linear Approximation]\label{defn:linear_approx}
    Let the function \(f(\cdot)\) be differentiable at \(\solution \in \R^n\). Then, for any \(\bm{y} \in \R^n\) we have
    \[
        f(\bm{y}) = f(\solution) + \innerprod{\grad f(\solution)}{\bm{y} - \solution} + o(\norm{\bm{y} - \solution}),
    \]
    where \(\grad f(\solution)\) is called the \emph{gradient} of the function \(f\) at \(\solution\). \(o(\cdot): [0, \infty) \to \R\) is a function of \(r \ge 0\) satisfying the conditions
    \[
        \lim_{r\to0} \frac{1}{r} o(r) = 0, \quad o(0) = 0. 
    \]
\end{defn}

Here we use the notation \(\norm{\cdot}\) for the standard \emph{Euclidean} norm in \(\R^n\):
\[
    \norm{\bm{x}} = \left[ \sum_{i=1}^n \left( x^{(i)} \right)^2 \right]^{1/2} = (\bm{x}^\top \bm{x})^{1/2} = \innerprod{\bm{x}}{\bm{x}}^{1/2},
\]
where \(\innerprod{\cdot}{\cdot}\) is the standard inner product in corresponding coordinate space. Note that
\[
    \forall \bm{x} \in \R^n,~\bm{y} \in \R^m, \bm{A} \in \R^{m \times n} \quad \Rightarrow \quad \innerprod{\bm{A}\bm{x}}{\bm{y}} \equiv \innerprod{\bm{x}}{\bm{A}^\top\bm{y}},
\] 

\begin{note}{Gradient}
    Consider the points: \(\bm{y}_i = \solution + \epsilon \bm{e}_i\), where \(\bm{e}_i\) is the \(i\)-th coordinate vector in \(\R^n\), and taking \(\epsilon \to 0\), we can get the following representation of the gradient:
    \begin{equation}\label{eq:grad}
        \grad f(\bm{x}) = \left( \pdv{f(\solution)}{x^{(1)}}, \dots, \pdv{f(\solution)}{x^{(n)}} \right)^\top
    \end{equation}
    Here we mention two important properties of the gradient. Denote by \(\mathscr{L}_f(\alpha)\) the \emph{(sub)level set} of \(f(\cdot)\):
    \[
        \mathscr{L}_f(\alpha) = \{ \bm{x} \in \R^n \mid f(\bm{x}) \le \alpha \}.
    \] 
    Consider the set of directions that are \emph{tangent} to \(\mathscr{L}_f(f(\solution))\) at \(\solution\):
    \[
        S_f(\solution) = \left\{
            \bm{s} \in \R^n \mid \bm{s} = \lim_{k \to \infty} \frac{\bm{y}_k - \solution}{\norm{\bm{y}_k - \solution}},
            ~\text{for some}~\{\bm{y}_k\} \to \solution ~\text{with}~ f(\bm{y}_k) = f(\solution) ~ \forall k
        \right\}.
    \]

    \begin{lemma}\label{lemma:s_grad}
        If \(\bm{s} \in S_f(\solution)\), then \(\innerprod{\grad f(\solution)}{\bm{s}} = 0\).
    \end{lemma}
    
    \begin{proof}[of Lemma~\ref{lemma:s_grad}]
        Since \(f(\bm{y}_k) = f(\solution)\), we can get
        \[
            f(\bm{y}_k) = f(\solution) + \innerprod{\grad f(\solution)}{\bm{y}_k - \solution} + o(\norm{\bm{y}_k - \solution}) = f(\solution).
        \]
        Therefore \(\innerprod{\grad f(\solution)}{\bm{y}_k - \solution} + o(\norm{\bm{y}_k - \solution}) = 0\). So dividing the equation by \(\norm{\bm{y}_k - \solution}\) and
        taking the limit as \(\bm{y}_k \to \solution\), we can obtain the result.
    \end{proof}

    \begin{boxnote}{The Fastest Local Decrease}
        Let \(\bm{s}\) be a direction in \(\R^n\), \(\norm{\bm{s}}\) = 1. Consider the local decrease of the function \(f(\cdot)\) along the direction \(\bm{s}\):
        \[
            \begin{aligned}
                \Delta(\bm{s}) = &\lim_{\alpha \to 0} \frac{1}{\alpha} [f(\solution + \alpha \bm{s}) - f(\solution)]\\
                &= \lim_{\alpha \to 0}\frac{1}{\alpha}[\alpha \innerprod{\grad f(\solution)}{\bm{s}} + o(\alpha)]\\
                &= \innerprod{\grad f(\solution)}{\bm{s}}
            \end{aligned}
        \] 
        Based on Cauchy-Schwarz inequality \( -\norm{\bm{x}}\cdot\norm{\bm{y}} \le \innerprod{\bm{x}}{\bm{y}} \le \norm{\bm{x}}\cdot\norm{\bm{y}} \), 
        we can obtain that \(\Delta(\bm{s}) = \innerprod{\grad f(\solution)}{\bm{s}} \ge -\norm{\grad f(\solution)}\). 
        Let us take \(\bar{\bm{s}} = -\grad f(\solution) / \norm{\grad f(\solution)}\), then
        \[
            \Delta(\bm{s}) = -\innerprod{\grad f(\solution)}{\grad f(\solution)} / \norm{\grad f(\solution)} = - \norm{\grad f(\solution)}.
        \]
        Thus, the direction of \(-\grad f(\solution)\) (\emph{the anti-gradient}) is the direction of the \emph{fastest local decrease} of the function \(f(\cdot)\) at point \(\solution\).
    \end{boxnote}
\end{note}

\begin{thm}[First-Order Optimality Condition]\label{thm:1st_order_optimal}
    Let \(\bm{x}^*\) be a local minimum of a differentiable function \(f(\cdot)\). Then
    \begin{equation}\label{eq:1st_order_optimal}
        \grad f(\bm{x}^*) = 0.
    \end{equation}
\end{thm}

\begin{proof}[of Theorem~\ref{thm:1st_order_optimal}]
    Since \(\bm{x}^*\) is a local minimum of \(f(\cdot)\),
    \[
      \exists r > 0,~\forall \bm{y} \in \R^n ~ \norm{\bm{y} - \bm{x}} < r,  \quad \text{s.t.}~ f(\bm{y}) \ge f(\bm{x}^*).
    \] 
    Since \(f(\cdot)\) is \emph{differentiable}, we can infer that
    \[
        f(\bm{y}) = f(\bm{x}^*) + \innerprod{\grad f(\bm{x}^*)}{\bm{y} - \bm{x}^*} + o(\norm{\bm{y} - \bm{x}^*}) \ge f(\bm{x}^*).
    \]
    Thus \(\forall \bm{s} \in \R^n\), we have \(\innerprod{\grad f(\bm{x}^*)}{\bm{s}} \ge 0\). By taking \(\bm{s} = -\grad f(\bm{x}^*)\), 
    we get \(-\norm{\grad f(\bm{x}^*})^2 \ge 0\). Hence, \(\grad f(\bm{x}) = 0\).
\end{proof}

\begin{defn}[Quadratic Approximation]\label{defn:quad_approx}
    Let \(f(\cdot)\) be twice differentiable at \(\solution\). Then
    \begin{equation}\label{eq:quad_approx}
        f(\bm{y}) = f(\solution) + \innerprod{\grad f(\solution)}{\bm{y} - \solution}
        + \frac{1}{2} \innerprod{\hess f(\solution)(\bm{y} - \solution)}{\bm{y} - \solution}
        + o(\norm{\bm{y} - \solution}^2),
    \end{equation}
    where \(\hess f(\solution)\) is the \emph{Hessian} matrix of function \(f\) at \(\solution\) 
    and \(\bm{o}(\cdot): [0, \infty) \R^n\) is a continuous vector function satisfying the condition
    \[
        \lim_{r\to0}\frac{1}{r} \norm{\bm{o}(r)} = 0.  
    \]
\end{defn}

\begin{boxnote}{Hessian Matrix}
    For Hessian matrix \(\hess f(\solution)\):
    \begin{enumerate}
        \item \(\hess f(\solution)^{(i, j)} = \pdv{f(\solution)}{x^{(i)}}{x^{(j)}}\).
        \item \(\hess f(\solution)\) is a \emph{symmetric} matrix: \(\hess f(\solution) = \left[\hess f(\solution)\right]^\top\).
        \item The Hessian can be regarded as a \emph{derivative} of the vector \(\grad f(\cdot)\):
              \[
                    \grad f(\bm{y}) = \grad f(\solution) + \hess f(\solution) (\bm{y} - \solution) + \bm{o}(\norm{\bm{y} - \solution}) \in \R^n
              \]
    \end{enumerate}
\end{boxnote}

Based on the quadratic approximation, we can write down the \emph{second-order optimality condition}.

\begin{thm}[Second-Order Optimality Condition]\label{thm:2nd_order_optimal}
    Let \(\bm{x}^*\) be a local minimum of a twice differentiable function \(f(\cdot)\). Then
    \begin{equation}
        \grad f(\bm{x}^*) = 0, \quad \hess f(\bm{x}^*) \succeq 0.
    \end{equation}
\end{thm}

\begin{proof}[of Theorem~\ref{thm:2nd_order_optimal}]
    Since \(\bm{x}^*\) is a local minimum of the function \(f(\cdot)\), 
    \[
        \exists r > 0,~\forall \bm{y} \in \R^n ~ \norm{\bm{y} - \bm{x}} < r,  \quad \text{s.t.}~ f(\bm{y}) \ge f(\bm{x}^*).
    \]

    In view of Theorem~\ref{thm:1st_order_optimal}, \(\grad f(\bm{x}^*) = 0\). Therefore, for any such \(\bm{y}\),
    \[
        f(\bm{y}) = f(\bm{x}^*) + \innerprod{\hess f(\bm{x}^*)(\bm{y} - \bm{x}^*)}{\bm{y} - \bm{x}^*} + o(\norm{\bm{y} - \bm{x}^*}^2) \ge f(\bm{x}^*).  
    \]
    Thus, \(\innerprod{\hess f(\bm{x}^*) \bm{s}}{\bm{s}} \ge 0\), for all \(\bm{s}\), \(\norm{\bm{s}} = 1\).
\end{proof}

Again, Theorem~\ref{thm:2nd_order_optimal} is a \emph{necessary} (\(2\)-nd-order) characteristic of a local minimum. The \emph{sufficient} condition is as follows.

\begin{thm}\label{thm:suff_2nd_order_optimal}
    Let a function \(f(\cdot)\) be twice differentiable on \(\R^n\) and let \(\bm{x}^* \in \R^n\) satisfy the following conditions:
    \begin{equation}
        \grad f(\bm{x}^*) = 0, \quad \hess f(\bm{x}^*) \succ 0.
    \end{equation}
    Then \(\bm{x}^*\) is a strict local minimum of \(f(\cdot)\)
\end{thm}

\begin{proof}[of Theorem~\ref{thm:suff_2nd_order_optimal}]
    In a small neighborhood of a point \(\bm{x}^*\) the function \(f(\cdot)\) can be represented as
    \[
        f(\bm{y}) = f(\bm{x}^*) + \frac{1}{2} \innerprod{\hess f(\bm{x}^*) (\bm{y} - \bm{x}^*)}{\bm{y} - \bm{x}^*} + o(\norm{\bm{y} - \bm{x}^*}^2)
    \]
    Since \(\lim_{r \to 0}\frac{o(r^2)}{r^2} = 0\), there exists a value \(\bar r > 0\) such that for all \(r \in [0, \bar r]\) we have
    \[
        \abs{o(r^2)} \le \frac{r^2}{4} \lambda_{\min} (\hess f(\bm{x}^*)).    
    \]

    In the view of our assumption, this eigenvalue is \emph{positive}. Therefore, for any \(\bm{y} \in \R^n\), \(0 \le \norm{\bm{y} - \bm{x}^*} \le \bar r\), we have
    \[
        \begin{aligned}
            f(\bm{y}) &\ge f(\bm{x}^*) + \frac{1}{2} \lambda_{\min} (\hess f(\bm{x}^*)) \norm{\bm{y} - \bm{x}^*}^2 + o(\norm{\bm{y} - \bm{x}^*}^2)\\
                      &\ge f(\bm{x}^*) + \frac{1}{4} \lambda_{\min} (\hess f(\bm{x}^*)) \norm{\bm{y} - \bm{x}^*}^2 > f(\bm{x}^*)
        \end{aligned}
    \]
\end{proof}

\subsection{Classes of Differentiable Functions}\label{subsec:classes-of-diff-funcs}
\begin{defn}\label{defn:classes-diff-funcs}
    Let \(Q\) be a subset of \(R^n\). We denote by \(C^{k,p}_L(Q)\) the class of functions with the following properties:
    \begin{itemize}
        \item any \(f \in C^{k, p}_L(Q)\) is \(k\) times continuously differentiable on \(Q\) 
        \item its \(p\)th derivative is \emph{Lipschitz} continuous on \(Q\) with constant \(L\):
            \[
                \norm{\grad^p f(\bm{x}) - \grad^p f(\bm y)} \le L \norm{\bm x - \bm y}, \quad \forall \bm x, ~\bm y \in Q
            \]
    \end{itemize}
\end{defn}

Clearly we always have \(p \le k\). If \(q > k\), then \(C_L^{q, p}(Q) \subset C_{L}^{k, p}(Q)\). Note also that these classes possess the following property:

\begin{thm}\label{thm:add-property}
    If \(f_1 \in C^{k,p}_{L_1}(Q)\), \(f_2 \in C^{k,p}_{L_2}(Q)\) and \(\alpha_1,~\alpha_2 \in \R\), then for
    \[
        L_3 = \abs{\alpha_1} L_1 + \abs{\alpha_2} L_2  
    \]
    we have \(\alpha_1 f_1 + \alpha_2 f_2 \in C^{k, p}_{L_3}(Q)\).
\end{thm}

One of the most important classes of differentiable functions is \(C_L^{1, 1}(\R^n)\), \emph{the class of functions with Lipschitz continuous gradient}, which means that
\[
    \norm{\grad f(\bm{x}) - \grad f({\bm{y}})} \le L \norm{\bm{x} - \bm{y}}, \quad \forall f \in C_{L}^{1, 1};~\bm{x},~\bm{y} \in \R^n. 
\]

\begin{lemma}\label{lemma:C21}
    A function \(f(\cdot)\) belongs to the class \(C_L^{2, 1}(\R^n) \subset C_L^{1, 1}(\R^n)\)
    if and only if for all \(\bm{x} \in \R^n\) we have
    \begin{equation}\label{eq:C21}
        \norm{\hess f(\bm{x})} \le L.
    \end{equation}
\end{lemma}

\begin{proof}[of Lemma.~\ref{lemma:C21}]
    pass
\end{proof}

For Eq.~\ref{eq:C21}, it can be rewritten as
\begin{equation}
    -L \bm{I}_n \preceq \hess f(\bm{x}) \preceq L \bm{I}_n.
\end{equation}

The next statement is important for the geometric interpretation of functions in \(C_L^{1, 1}(\R^n)\)
\begin{lemma}\label{lemma:geometric-interpretation-C11L}
    Let \(f \in C_L^{1, 1}(\R^n)\). Then we have
    \begin{equation}
        \abs{f(\bm{y}) - f(\bm{x}) - \innerprod{\grad f(\bm{x})}{\bm{y} - \bm{x}}} \le \frac{L}{2} \norm{\bm{y} - \bm{x}}^2.
    \end{equation}
\end{lemma}

\begin{proof}[of Lemma.~\ref{lemma:geometric-interpretation-C11L}]
    pass
\end{proof}

The second main class of functions is type \(C^{2, 2}_M\), \emph{the class of twice differentiable functions with Lipschitz continuous Hessian}.
Recall that for \(f \in C^{2,2}_M(\R^n)\), we have
\[
    \norm{\hess f(\bm{x}) - \hess f(\bm{y})} \le M \norm{\bm{x} - \bm{y}}, \quad \forall \bm{x},~\bm{y} \in \R^n. 
\]

\begin{lemma}
    Let \(f \in C^{2,2}_M(\R^n)\). The for all \(\bm{x}, ~\bm{y} \in \R^n\) we have
    \begin{equation}\label{eq:C22M_1}
        \norm{\grad f(\bm{y}) - \grad f(\bm{x}) - \hess f(\bm{x})(\bm{y} -\bm{x})} \le \frac{M}{2} \norm{\bm{y} - \bm{x}}^2.
    \end{equation}
    \begin{equation}\label{eq:C22M_2}
        \abs{f(\bm{y}) - f(\bm{x}) - \innerprod{\grad f(\bm{x})}{\bm{y} - \bm{x}} - \frac{1}{2} \innerprod{\hess f(\bm{x})(\bm{y} -\bm{x})}{\bm{y} - \bm{x}}} \le \frac{M}{6} \norm{\bm{y} - \bm{x}}^3.
    \end{equation}
\end{lemma}

\begin{coro}
    Let \(f \in C^{2,2}_M(\R^n)\) and \(\bm{x},~\bm{y} \in \R^n\) with \(\norm{\bm{y} - \bm{x}} = r\). Then
    \[
        \hess f(\bm{x}) - Mr \bm{I}_n \preceq \hess f(\bm{y}) \preceq \hess f(\bm{x}) + Mr\bm{I}_n.  
    \]
\end{coro}

\subsection{The Gradient Method}

As we have already seen, the \emph{antigradient} is the direction of locally steepest
descent of a differentiable function. So let's consider the following strategy.

\begin{boxnote}{Gradient Method}
    \textbf{Choose} \(\xB_0 \in \R^n\).

    \textbf{Iterate} \(\xB_{k+1} = \xB_k - h_k \grad f(\xB_k),~k=0, 1, \dots\).
\end{boxnote}

This scheme is called \emph{Gradient Method}. The scalar factor \(h_k\) are called \emph{step sizes} and they
are positive.

There are many variants of this method, which differ one from another by the \emph{step-size strategy}.
\begin{enumerate}
    \item The sequence \(\{h_k\}_{k=0}^\infty\) is chosen \emph{in advance}. For instance
        \[
            \begin{aligned}
                &h_k = h > 0,~\text{(constant step)}
                &h_k = \frac{h}{\sqrt{k+1}}
            \end{aligned}  
        \]
        This is the simplest one. It is often used in the context of Convex Optimization. In this framework, the behavior of functions
        is much predictable than in general nonlinear case.
    \item \emph{Full relaxation}:
        \[
            h_k = \mathop{\arg\min}_{h \ge 0} f(\xB_k - h \grad f(\xB_k)).  
        \]
        This strategy is completely \emph{theoritical}. It is never used in practice since even in one-dimensional case we cannot
        find the \emph{exact minimum in finite time}.
    \item The \emph{Armijo} rule: Find \(\xB_{k+1} =\xB_k - h\grad f(\xB_k)\) with \(h > 0\) such that
        \[
            \begin{aligned}
                \alpha \innerprod{\grad f(\xB_k)}{\xB_k - \xB_{k+1}} &\le f(\xB_k) - f(\xB_{k+1})\\
                \beta \innerprod{\grad f(\xB_k)}{\xB_k - \xB_{k+1}} &\ge f(\xB_k) - f(\xB_{k+1})
            \end{aligned}  
        \]
        where \(0 < \alpha < \beta < 1\) are some fixed parameters.
\end{enumerate}

The third strategy is used in the majority of practical algorithms. It has the following geometric interpretation.
Let us fix \(\xB \in \R^n\) assuming that \(\grad f(\xB) \ne 0\). Consider the following function of one variable:
\[
    \phi(h) = f(\xB - h \grad f(\xB)), \quad h \ge 0.  
\]

Then the step-size values acceptable for this strategy belong to thee part of the graph of \(\phi\) which is located between \(2\) linear functions:
\[
    \phi_1(h) = f(\xB) - \alpha h \norm{\grad f(\xB)}^2, \quad \phi_2(h) = f(\xB) - \beta h \norm{\grad f(\xB)}^2.  
\]
Note that \(\phi(0) = \phi_1(0) = \phi_2(0)\) and \(\phi'(0) < \phi_2'(0) < \phi_1'(0) < 0\). Therefore, the acceptable values exist unless \(\phi(\cdot)\)
is not bounded below.

Let us estimate the performace of Gradient Method. Consider the problem
\begin{equation}\label{eq:grad-problem}
    \min_{\xB \in \R^n} f(\xB),
\end{equation}
with \(f \in C_{L}^{1,1}(\R^n)\), and assume that \(f(\cdot)\) is bounded below on \(\R^n\).

For one gradient step, consider \(\yB = \xB - h \grad f(\xB)\). Then, in view of Lemma.~\ref{lemma:geometric-interpretation-C11L}, we have
\begin{equation}\label{eq:one-step-grad}
    \begin{aligned}
        f(\yB) &\le f(\xB) + \innerprod{\grad f(\xB)}{\yB - \xB} + \frac{L}{2} \norm{\yB - \xB}^2 \\
        &= f(\xB) - h\left(1 - \frac{h}{2}L\right)\norm{\grad f(\xB)}^2.
    \end{aligned}
\end{equation}

In order to get the best upper bound for the possible decrease of the objective function, we have to solve the following one-dimensional problem:
\[
    \Delta(h) = -h \left(1 - \frac{h}{2}L\right) \to \min_h.  
\]
Computing the derivative of this function, we conclude that the optimal step size \(h^* = \frac{1}{L}\), which is a minimum of \(\Delta(h)\) since 
\(\Delta''(h) = L > 0\).

Therefore, we can get that one step of the Gradient Method decreases the value of the objective function at least as follows:
\[
    f(\yB) \le f(\xB) - \frac{1}{2L} \norm{\grad f(\xB)}^2.  
\]

Let \(\xB_{k+1} = \xB_k - h_k \grad f(\xB_k)\). 
\begin{enumerate}
    \item {
        \textbf{For the constant step strategy}: \(h_k = h\). Then we have
        \[
            f(\xB_k) - f(\xB_{k+1}) \ge h\left(1 - \frac{1}{2}Lh\right) \norm{\grad f(\xB_k)}^2.  
        \]
        If we choose \(h_k = \frac{2\alpha}{L}\) with \(\alpha \in (0, 1)\), then
        \[
            f(\xB_k) - f(\xB_{k+1}) \ge \frac{2}{L} \alpha (1 - \alpha) \norm{\grad f(\xB_k)}^2.  
        \]
    }
    \item {
        \textbf{For the full relaxation strategy}: 
        \[
            f(\xB_k) - f(\xB_{k+1}) \ge \frac{1}{2L} \norm{\grad f(\xB_k)}^2  
        \]
    }
    \item {
        \textbf{For the Armijo rule}:
        \[
            f(\xB_k) - f(\xB_{k+1}) \le \beta \innerprod{\grad f(\xB_k)}{\xB_k - \xB_{k+1}} = \beta h_k \norm{\grad f(\xB_k)}^2.  
        \]
        From \ref{eq:one-step-grad}, we obtain
        \[
            f(\xB_k) - f(\xB_{k+1}) \ge h_k \left(1 - \frac{h_k}{2}L\right) \norm{\grad f(\xB_k)}^2.  
        \]
        Thus, \(h_k \ge \frac{2}{L}(1 - \beta)\). Furthermore, 
        \[
            f(\xB_k) - f(\xB_{k+1}) \ge \alpha \innerprod{\grad f(\xB_k)}{\xB_k - \xB_{k+1}} = \alpha h_k \norm{\grad f(\xB_k)}^2.
        \] `'
        Combining this inequality with the previous one, we conclude that
        \[
            f(\xB_k) - f(\xB_{k+1}) \ge \frac{2}{L}\alpha (1 - \beta) \norm{\grad f(\xB_k)}^2.  
        \]
    }   
\end{enumerate}

Above all, we have proved that we have
\begin{equation}\label{eq:general-one-step-grad}
    f(\xB_k) - f(\xB_{k+1}) \ge \frac{\omega}{L} \norm{\grad f(\xB_k)}^2,
\end{equation} 
where \(\omega\) is some positive constant.

Now we are already to estimate the performace of Gradient Method. Summing up the 
inequalities \ref{eq:general-one-step-grad} for \(k = 0, \dots, N\), we obtain
\begin{equation}\label{eq:sum-grad-desc}
    \frac{\omega}{L} \sum_{k=0}^N \norm{\grad f(\xB_k)}^2 \le f(\xB_0) - f(\xB_{N+1}) \le f(\xB_0) - f^*,  
\end{equation}
where \(f^*\) is lower bounds for the values of objective function in the problem \ref{eq:grad-problem}. As 
a simple consequence of bound \ref{eq:sum-grad-desc}, we have
\[
    \lim_{k\to \infty}\norm{\grad f(\xB_k)} = 0  
\]

However, we also say something about the \emph{rate of convergence}. We define
\[
    g_N^* = \min_{0 \le k \le N} \norm{\grad f(\xB_k)}.  
\]

Then in view of \ref{eq:sum-grad-desc}, we come to the following inequality:
\begin{equation}\label{eq:rate-of-convergence-gN}
    g_N^* \le \frac{1}{N+1}\left[\frac{L}{\omega} (f(\xB_0) - f^*)\right]^{1/2}
\end{equation}

The right-hand side of this inequality describes the rate of convergence of the sequence of the sequence \(\{g_N^*\}\) to zero.
Our current goal is to approach a minimum of the optimization problem \ref{eq:grad-problem}.
However, in general, even this goal is unreachable for the Gradient Method.

\begin{example}
    Consider the following function of two variables:
    \[
        f(\xB) \equiv f(x^{(1)}, x^{(2)}) = \frac{1}{2} \left(x^{(1)}\right)^2 + \frac{1}{4} \left(x^{(2)}\right)^4
        - \frac{1}{2}\left(x^{(2)}\right)^2. 
    \]
    The gradient of this function is \(\grad f(\xB) = \left(x^{(1)}, \left(x^{(2)}\right)^3 - x^{(2)}\right)^\top\).
    There are only three points which can pretend to be a local minimum of this function:
    \[
        \xB_1^* = (0,0), \quad \xB_2^* = (0, -1), \quad \xB_3^* = (0, 1).  
    \]
    Computing the Hessian of this function,
    \[
        \hess f(\xB) = \begin{pmatrix}
            1 & 0 \\
            0 & 3\left(x^{(2)}\right)^2 - 1
        \end{pmatrix},  
    \]
    we conclude that \(\xB^*_2\) and \(\xB^*_3\) are isolated local minima, but \(\xB_1^*\) is only a \emph{stationary point} of this function.
    In deed, \(f(\xB^*_1) = 0\) and \(f(\xB_1^* + \epsilon \eB_2) = \frac{\epsilon^4}{4} - \frac{\epsilon^2}{2} < 0\) for \(\epsilon\) small enough.

    Let's consider now trajectory of the Gradient Method which starts at \(\xB_0 = (1, 0)\). The entire sequence will have second coordinate equal to zero.
    This means that this sequence converges to \(\xB^*_1\).
\end{example}

The \emph{rate of convergence} delivers an \emph{upper} complexity bound for the corresponding problem class. Such a bound is always justified by some numerical method.
A method for which the upper complexity bound is proportional to the \emph{lower} complexity bound of the problem class is said to be \emph{optimal}.

Consider the following problem class \(\mathscr{G}_*\).
\begin{boxnote}{Problem class \(\mathscr{G}_*\)}
    \begin{itemize}
        \item \textbf{Model}: 
            \begin{enumerate}
                \item Unconstrained minimization.
                \item \(f \in C^{1, 1}_L(\R^n)\).
                \item \(f(\cdot)\) is bounded below by the value \(f^*\).
            \end{enumerate}
        \item \textbf{Oracle}: First-Order Black Box
        \item \textbf{\(\varepsilon\)-solution}: \(f(\bar{\xB}) \le f(\xB_0),~\norm{\grad f(\bar{\xB})} \le \epsilon\)
    \end{itemize}
\end{boxnote}

Note that the inequality \ref{eq:rate-of-convergence-gN} can be used in order to obtain an upper bound for the number of steps (= calls of oracle), which is necessary to
find a point where the norm of the gradient is small. For that, let us write down the following inequality:
\begin{equation}\label{eq:upper-bound-grad}
    g_N^* \le \frac{1}{N+1}\left[\frac{L}{\omega} (f(\xB_0) - f^*)\right]^{1/2} \le \epsilon.
\end{equation}
Therefore, if \(N + 1 \ge \frac{L}{\omega \epsilon^2} (f(\xB_0) - f^*)\), then we necessarily have \(g_N^* \le \epsilon\).
The lower complexity bound for the class \(\mathscr{G}_*\) is unknown.

\begin{thm}
    Let the function \(f(\cdot)\) satisfy our assumptions and let the starting point \(\xB_0\) be close enough to a strict local minimum \(\xB^*\):
    \[
        r_0 = \norm{\xB_0 - \xB^*} < \bar{r} = \frac{2\mu}{M}.  
    \]
    Then the Gradient Method with step size converges as follows:
    \[
        \norm{\xB_k - \xB^*} \le \frac{\bar{r}r_0}{\bar{r} - r_0} \left(1 - \frac{2\mu}{L+3\mu}\right)^k. 
    \]
\end{thm}
This type of rate of convergence is called \emph{linear}.

\subsection{Newton's Method}\label{subsect:newton-method}
Newton's Method is widely known as a technique for finding a root of univariate function. Let \(\phi(\cdot): \R \to \R\). Consider the equation
\[
    \phi(t^*) = 0.  
\]
Assume that we know some \(t \in \R\) which is close enough to \(t^*\). Note that
\[
    \phi(t + \Delta t) = \phi(t) + \phi'(t) \Delta t + o(|\Delta t|).  
\]
Therefore, the solution can be approximated by the solution of the following \emph{linear} equation:
\[
    \phi(t) + \phi'(t) \Delta t = 0.  
\]
Under some conditions, we can expect the displacement \(\Delta t\) to be a good approximation to the optimal displacement \(\Delta t^* = t^* - t\).
Converting this idea into an algorithm, we get the process
\[
    t_{k+1} = t_k - \frac{\phi(t_k)}{\phi'(t_k)}.  
\]

This scheme can be naturally extended to the problem of finding a solution to a system of nonlinear equations
\[
    \bm{F}(\xB) = 0, ~\xB \in \R^n, ~\bm{F}(\cdot):\R^n \to \R^n.
\]
In this case, the scheme is as follows:
\[
    \xB_{k+1} = \xB_k - [\bm{F}'(\xB_k)]^{-1}\bm{F}(\xB_k).  
\]

Finally, in view of Theorem.~\ref{thm:1st_order_optimal}, we can replace th unconstrained minimization problem \ref{eq:grad-problem} by the problem of \emph{finding a root of the
nonlinear system}
\[
    \grad f(\xB) = 0.  
\]
This replacement is not completely equivalent, but it works in \emph{nondegenerate} situations. 
In this case, the Newton system is as follows:
\[
    \grad f(\xB) + \hess f(\xB) \Delta \xB = 0.  
\]
Hence, the Newton's Method for optimization problems can be written in the following form:
\[
    \xB_{k+1} = \xB_{k} - \left[\hess f(\xB_k)\right]^{-1} \grad f(\xB_k). 
\]

\begin{boxnote}{Newton's Method from Quadratic Approximation}
    Consider the approximation
    \[
        \phi(\xB) = f(\xB_k) + \innerprod{\grad f(\xB_k)}{\xB - \xB_k} + \frac{1}{2} \innerprod{\hess f(\xB_k)(\xB - \xB_k)}{\xB - \xB_k}.
    \]
    Assume that \(\hess f(\xB_k) \succ 0\). Then we can choose \(\xB_{k+1}\) as the minimizer of the quadratic function \(\phi(\cdot)\), This means that
    \[
        \grad \phi(\xB_{k+1}) = \grad f(\xB_k) + \hess f(\xB_k) (\xB_{k+1} - \xB_k) = 0,  
    \]
    and we come again to Newton's process.
\end{boxnote}

The convergence of the Newton's Method in a neighborhood of a strict local minimum is very fast. Nevertheless, this method has two serious drawbacks:
\begin{enumerate}
    \item It can break down if \(\hess f(\xB_k)\) is degenerate.
    \item Newton's process can diverge.
\end{enumerate}

\begin{example}
    Consider following univariate function
    \[
        \phi(t) = \frac{t}{\sqrt{1 + t^2}}.  
    \]
    Clearly, \(t^* = 0\). Note that 
    \[
        \phi'(t) = \frac{1}{(1 + t^2)^{3/2}}.  
    \]
    Therefore Newton's process is as follows:
    \[
        t_{k+1} = t_k - \frac{\phi(t_k)}{\phi'(t_k)} = t_k - \frac{t_k}{\sqrt{1 + t_k^2}} \cdot [1 + t_k^2]^{3/2} = -t_k^3. 
    \]
    Thus, if \(\abs{t_0} < 1\), then this method converges and the convergence is extremely fast. The points \(\pm 1\) are oscillation points of this scheme.
    If \(\abs{t}_0 > 1\), then this method diverges.
\end{example}

In order to avoid a possible diverge, in practice we can apply the \emph{damped Newton's method}:
\[
    \xB_{k+1} = \xB_k - h_k [\hess f(\xB_k)]^{-1} \grad f(\xB_k),  
\]
where \(h_k > 0\) is a step size parameter. At the initial stage of the method we can use the same step size strategies as for the gradient scheme. At the final stage, 
it is reasonable to choose \(h_k = 1\). Another possibility for ensuring the global convergence of the scheme consists in using \emph{Cubic Regularization}, This approach will
be studied in the later chapters.

\begin{thm}
    Let the function \(f(\cdot)\) satisfy our assumptions. Suppose that the initial starting point \(\xB_0\) is close enough to \(\xB^*\):
    \[
        \norm{\xB_0 - \xB^*} \le \bar{r} = \frac{2\mu}{3M}.   
    \]
    Then \(\norm{\xB_0 - \xB^*} \le \bar{r}\) for all \(k\) and the Newton's Method converges quadratically:
    \[
        \norm{\xB_{k+1} - \xB^*} \le \frac{M \norm{\xB_k - \xB^*}}{2(\mu - M\norm{\xB_k -\xB^*})}.  
    \]
\end{thm}

\begin{enumerate}
    \item \textbf{Sublinear rate}
    \item \textbf{Linear rate}
    \item \textbf{Quadratic rate}
\end{enumerate}