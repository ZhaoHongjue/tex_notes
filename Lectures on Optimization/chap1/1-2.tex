\section{Local Methods in Unconstrained Minimization}\label{sec:local_methods_in_unconstrained_min}

In this section, we consider several methods for solving the following unconstrained minimization problem:
\begin{equation}\label{eq:unconstrained_nonlinear_problem}
    \min_{\bm{x} \in \R^n} f(\bm{x}),
\end{equation}
where \(f(\cdot)\) is a smooth function.

\subsection{Relaxation and Approximation}\label{subsec:relax_approx}

The simplest goal in general Nonlinear Optimization consists in \emph{finding a local minimum of a differentiable function}. 
The majority of methods in general Nonlinear Optimization are based on the idea of \emph{relaxation}.

\begin{defn}[Relaxation]\label{defn:relaxation}
    A sequence of real numbers \(\{ a_k \}_{k=0}^\infty\) is called a \emph{relaxation sequence} if 
    \[
        a_{k+1} \le a_k \quad \forall k > 0. 
    \]
\end{defn}

In order to deal with problem~\ref{eq:unconstrained_nonlinear_problem}, most of methods generate a relaxation sequence of function values \(\{ f(\bm{x}_k) \}_{k=0}^{\infty}\):
\[
    f(\bm{x}_{k+1}) \le f(\bm{x}_k), \quad k = 0, 1, \dots ~.  
\]
This rule has the following important advantages:
\begin{enumerate}
    \item If \(f(\cdot)\) is bounded below on \(\R^n\), then the sequence \(\{ f(\bm{x}_k) \}_{k=0}^{\infty}\) converges.
    \item In any case, we improve the initial value of the objective function.
\end{enumerate}

In the meanwhile, there is another concept which is essential to implement the idea of relaxation. That is \emph{approximation}.
\begin{defn}[Approximation]\label{defn:approx}
    To approximate means to replace an initial complex object by a simper one which is close to the original in terms of its properties.
\end{defn}

In Nonlinear Optimization, we usually apply \emph{local approximation} based on derivatives of nonlinear functions. These are \(1\)-st- and \(2\)-nd-order approximations (or, the linear and quadratic approximations).

\begin{defn}[Linear Approximation]\label{defn:linear_approx}
    Let the function \(f(\cdot)\) be differentiable at \(\solution \in \R^n\). Then, for any \(\bm{y} \in \R^n\) we have
    \[
        f(\bm{y}) = f(\solution) + \innerprod{\grad f(\solution)}{\bm{y} - \solution} + o(\norm{\bm{y} - \solution}),
    \]
    where \(\grad f(\solution)\) is called the \emph{gradient} of the function \(f\) at \(\solution\). \(o(\cdot): [0, \infty) \to \R\) is a function of \(r \ge 0\) satisfying the conditions
    \[
        \lim_{r\to0} \frac{1}{r} o(r) = 0, \quad o(0) = 0. 
    \]
\end{defn}

Here we use the notation \(\norm{\cdot}\) for the standard \emph{Euclidean} norm in \(\R^n\):
\[
    \norm{\bm{x}} = \left[ \sum_{i=1}^n \left( x^{(i)} \right)^2 \right]^{1/2} = (\bm{x}^\top \bm{x})^{1/2} = \innerprod{\bm{x}}{\bm{x}}^{1/2},
\]
where \(\innerprod{\cdot}{\cdot}\) is the standard inner product in corresponding coordinate space. Note that
\[
    \forall \bm{x} \in \R^n,~\bm{y} \in \R^m, \bm{A} \in \R^{m \times n} \quad \Rightarrow \quad \innerprod{\bm{A}\bm{x}}{\bm{y}} \equiv \innerprod{\bm{x}}{\bm{A}^\top\bm{y}},
\] 

\begin{note}{Gradient}
    Consider the points: \(\bm{y}_i = \solution + \epsilon \bm{e}_i\), where \(\bm{e}_i\) is the \(i\)-th coordinate vector in \(\R^n\), and taking \(\epsilon \to 0\), we can get the following representation of the gradient:
    \begin{equation}\label{eq:grad}
        \grad f(\bm{x}) = \left( \pdv{f(\solution)}{x^{(1)}}, \dots, \pdv{f(\solution)}{x^{(n)}} \right)^\top
    \end{equation}
    Here we mention two important properties of the gradient. Denote by \(\mathscr{L}_f(\alpha)\) the \emph{(sub)level set} of \(f(\cdot)\):
    \[
        \mathscr{L}_f(\alpha) = \{ \bm{x} \in \R^n \mid f(\bm{x}) \le \alpha \}.
    \] 
    Consider the set of directions that are \emph{tangent} to \(\mathscr{L}_f(f(\solution))\) at \(\solution\):
    \[
        S_f(\solution) = \left\{
            \bm{s} \in \R^n \mid \bm{s} = \lim_{k \to \infty} \frac{\bm{y}_k - \solution}{\norm{\bm{y}_k - \solution}},
            ~\text{for some}~\{\bm{y}_k\} \to \solution ~\text{with}~ f(\bm{y}_k) = f(\solution) ~ \forall k
        \right\}.
    \]

    \begin{lemma}\label{lemma:s_grad}
        If \(\bm{s} \in S_f(\solution)\), then \(\innerprod{\grad f(\solution)}{\bm{s}} = 0\).
    \end{lemma}
    
    \begin{proof}[of Lemma~\ref{lemma:s_grad}]
        Since \(f(\bm{y}_k) = f(\solution)\), we can get
        \[
            f(\bm{y}_k) = f(\solution) + \innerprod{\grad f(\solution)}{\bm{y}_k - \solution} + o(\norm{\bm{y}_k - \solution}) = f(\solution).
        \]
        Therefore \(\innerprod{\grad f(\solution)}{\bm{y}_k - \solution} + o(\norm{\bm{y}_k - \solution}) = 0\). So dividing the equation by \(\norm{\bm{y}_k - \solution}\) and
        taking the limit as \(\bm{y}_k \to \solution\), we can obtain the result.
    \end{proof}

    \begin{colorboxnote}{The Fastest Local Decrease}
        Let \(\bm{s}\) be a direction in \(\R^n\), \(\norm{\bm{s}}\) = 1. Consider the local decrease of the function \(f(\cdot)\) along the direction \(\bm{s}\):
        \[
            \begin{aligned}
                \Delta(\bm{s}) = &\lim_{\alpha \to 0} \frac{1}{\alpha} [f(\solution + \alpha \bm{s}) - f(\solution)]\\
                &= \lim_{\alpha \to 0}\frac{1}{\alpha}[\alpha \innerprod{\grad f(\solution)}{\bm{s}} + o(\alpha)]\\
                &= \innerprod{\grad f(\solution)}{\bm{s}}
            \end{aligned}
        \] 
        Based on Cauchy-Schwarz inequality \( -\norm{\bm{x}}\cdot\norm{\bm{y}} \le \innerprod{\bm{x}}{\bm{y}} \le \norm{\bm{x}}\cdot\norm{\bm{y}} \), 
        we can obtain that \(\Delta(\bm{s}) = \innerprod{\grad f(\solution)}{\bm{s}} \ge -\norm{\grad f(\solution)}\). 
        Let us take \(\bar{\bm{s}} = -\grad f(\solution) / \norm{\grad f(\solution)}\), then
        \[
            \Delta(\bm{s}) = -\innerprod{\grad f(\solution)}{\grad f(\solution)} / \norm{\grad f(\solution)} = - \norm{\grad f(\solution)}.
        \]
        Thus, the direction of \(-\grad f(\solution)\) (\emph{the anti-gradient}) is the direction of the \emph{fastest local decrease} of the function \(f(\cdot)\) at point \(\solution\).
    \end{colorboxnote}
\end{note}

\begin{thm}[First-Order Optimality Condition]\label{thm:1st_order_optimal}
    Let \(\bm{x}^*\) be a local minimum of a differentiable function \(f(\cdot)\). Then
    \begin{equation}\label{eq:1st_order_optimal}
        \grad f(\bm{x}^*) = 0.
    \end{equation}
\end{thm}

\begin{proof}[of Theorem~\ref{thm:1st_order_optimal}]
    Since \(\bm{x}^*\) is a local minimum of \(f(\cdot)\),
    \[
      \exists r > 0,~\forall \bm{y} \in \R^n ~ \norm{\bm{y} - \bm{x}} < r,  \quad \text{s.t.}~ f(\bm{y}) \ge f(\bm{x}^*).
    \] 
    Since \(f(\cdot)\) is \emph{differentiable}, we can infer that
    \[
        f(\bm{y}) = f(\bm{x}^*) + \innerprod{\grad f(\bm{x}^*)}{\bm{y} - \bm{x}^*} + o(\norm{\bm{y} - \bm{x}^*}) \ge f(\bm{x}^*).
    \]
    Thus \(\forall \bm{s} \in \R^n\), we have \(\innerprod{\grad f(\bm{x}^*)}{\bm{s}} \ge 0\). By taking \(\bm{s} = -\grad f(\bm{x}^*)\), 
    we get \(-\norm{\grad f(\bm{x}^*})^2 \ge 0\). Hence, \(\grad f(\bm{x}) = 0\).
\end{proof}

\begin{defn}[Quadratic Approximation]\label{defn:quad_approx}
    Let \(f(\cdot)\) be twice differentiable at \(\solution\). Then
    \begin{equation}\label{eq:quad_approx}
        f(\bm{y}) = f(\solution) + \innerprod{\grad f(\solution)}{\bm{y} - \solution}
        + \frac{1}{2} \innerprod{\hess f(\solution)(\bm{y} - \solution)}{\bm{y} - \solution}
        + o(\norm{\bm{y} - \solution}^2),
    \end{equation}
    where \(\hess f(\solution)\) is the \emph{Hessian} matrix of function \(f\) at \(\solution\) 
    and \(\bm{o}(\cdot): [0, \infty) \R^n\) is a continuous vector function satisfying the condition
    \[
        \lim_{r\to0}\frac{1}{r} \norm{\bm{o}(r)} = 0.  
    \]
\end{defn}

\begin{colorboxnote}{Hessian Matrix}
    For Hessian matrix \(\hess f(\solution)\):
    \begin{enumerate}
        \item \(\hess f(\solution)^{(i, j)} = \pdv{f(\solution)}{x^{(i)}}{x^{(j)}}\).
        \item \(\hess f(\solution)\) is a \emph{symmetric} matrix: \(\hess f(\solution) = \left[\hess f(\solution)\right]^\top\).
        \item The Hessian can be regarded as a \emph{derivative} of the vector \(\grad f(\cdot)\):
              \[
                    \grad f(\bm{y}) = \grad f(\solution) + \hess f(\solution) (\bm{y} - \solution) + \bm{o}(\norm{\bm{y} - \solution}) \in \R^n
              \]
    \end{enumerate}
\end{colorboxnote}

Based on the quadratic approximation, we can write down the \emph{second-order optimality condition}.

\begin{thm}[Second-Order Optimality Condition]\label{thm:2nd_order_optimal}
    Let \(\bm{x}^*\) be a local minimum of a twice differentiable function \(f(\cdot)\). Then
    \begin{equation}
        \grad f(\bm{x}^*) = 0, \quad \hess f(\bm{x}^*) \succeq 0.
    \end{equation}
\end{thm}

\begin{proof}[of Theorem~\ref{thm:2nd_order_optimal}]
    Since \(\bm{x}^*\) is a local minimum of the function \(f(\cdot)\), 
    \[
        \exists r > 0,~\forall \bm{y} \in \R^n ~ \norm{\bm{y} - \bm{x}} < r,  \quad \text{s.t.}~ f(\bm{y}) \ge f(\bm{x}^*).
    \]

    In view of Theorem~\ref{thm:1st_order_optimal}, \(\grad f(\bm{x}^*) = 0\). Therefore, for any such \(\bm{y}\),
    \[
        f(\bm{y}) = f(\bm{x}^*) + \innerprod{\hess f(\bm{x}^*)(\bm{y} - \bm{x}^*)}{\bm{y} - \bm{x}^*} + o(\norm{\bm{y} - \bm{x}^*}^2) \ge f(\bm{x}^*).  
    \]
    Thus, \(\innerprod{\hess f(\bm{x}^*) \bm{s}}{\bm{s}} \ge 0\), for all \(\bm{s}\), \(\norm{\bm{s}} = 1\).
\end{proof}

Again, Theorem~\ref{thm:2nd_order_optimal} is a \emph{necessary} (\(2\)-nd-order) characteristic of a local minimum. The \emph{sufficient} condition is as follows.

\begin{thm}\label{thm:suff_2nd_order_optimal}
    Let a function \(f(\cdot)\) be twice differentiable on \(\R^n\) and let \(\bm{x}^* \in \R^n\) satisfy the following conditions:
    \begin{equation}
        \grad f(\bm{x}^*) = 0, \quad \hess f(\bm{x}^*) \succ 0.
    \end{equation}
    Then \(\bm{x}^*\) is a strict local minimum of \(f(\cdot)\)
\end{thm}

\begin{proof}[of Theorem~\ref{thm:suff_2nd_order_optimal}]
    In a small neighborhood of a point \(\bm{x}^*\) the function \(f(\cdot)\) can be represented as
    \[
        f(\bm{y}) = f(\bm{x}^*) + \frac{1}{2} \innerprod{\hess f(\bm{x}^*) (\bm{y} - \bm{x}^*)}{\bm{y} - \bm{x}^*} + o(\norm{\bm{y} - \bm{x}^*}^2)
    \]
    Since \(\lim_{r \to 0}\frac{o(r^2)}{r^2} = 0\), there exists a value \(\bar r > 0\) such that for all \(r \in [0, \bar r]\) we have
    \[
        \abs{o(r^2)} \le \frac{r^2}{4} \lambda_{\min} (\hess f(\bm{x}^*)).    
    \]

    In the view of our assumption, this eigenvalue is \emph{positive}. Therefore, for any \(\bm{y} \in \R^n\), \(0 \le \norm{\bm{y} - \bm{x}^*} \le \bar r\), we have
    \[
        \begin{aligned}
            f(\bm{y}) &\ge f(\bm{x}^*) + \frac{1}{2} \lambda_{\min} (\hess f(\bm{x}^*)) \norm{\bm{y} - \bm{x}^*}^2 + o(\norm{\bm{y} - \bm{x}^*}^2)\\
                      &\ge f(\bm{x}^*) + \frac{1}{4} \lambda_{\min} (\hess f(\bm{x}^*)) \norm{\bm{y} - \bm{x}^*}^2 > f(\bm{x}^*)
        \end{aligned}
    \]
\end{proof}

\subsection{Classes of Differentiable Functions}\label{subsec:classes-of-diff-funcs}
\begin{defn}\label{defn:classes-diff-funcs}
    Let \(Q\) be a subset of \(R^n\). We denote by \(C^{k,p}_L(Q)\) the class of functions with the following properties:
    \begin{itemize}
        \item any \(f \in C^{k, p}_L(Q)\) is \(k\) times continuously differentiable on \(Q\) 
        \item its \(p\)th derivative is \emph{Lipschitz} continuous on \(Q\) with constant \(L\):
            \[
                \norm{\grad^p f(\bm{x}) - \grad^p f(\bm y)} \le L \norm{\bm x - \bm y}, \quad \forall \bm x, ~\bm y \in Q
            \]
    \end{itemize}
\end{defn}

Clearly we always have \(p \le k\). If \(q > k\), then \(C_L^{q, p}(Q) \subset C_{L}^{k, p}(Q)\). Note also that these classes possess the following property:

\begin{thm}\label{thm:add-property}
    If \(f_1 \in C^{k,p}_{L_1}(Q)\), \(f_2 \in C^{k,p}_{L_2}(Q)\) and \(\alpha_1,~\alpha_2 \in \R\), then for
    \[
        L_3 = \abs{\alpha_1} L_1 + \abs{\alpha_2} L_2  
    \]
    we have \(\alpha_1 f_1 + \alpha_2 f_2 \in C^{k, p}_{L_3}(Q)\).
\end{thm}

One of the most important classes of differentiable functions is \(C_L^{1, 1}(\R^n)\), \emph{the class of functions with Lipschitz continuous gradient}, which means that
\[
    \norm{\grad f(\bm{x}) - \grad f({\bm{y}})} \le L \norm{\bm{x} - \bm{y}}, \quad \forall f \in C_{L}^{1, 1};~\bm{x},~\bm{y} \in \R^n. 
\]

\begin{lemma}\label{lemma:C21}
    A function \(f(\cdot)\) belongs to the class \(C_L^{2, 1}(\R^n) \subset C_L^{1, 1}(\R^n)\)
    if and only if for all \(\bm{x} \in \R^n\) we have
    \begin{equation}\label{eq:C21}
        \norm{\hess f(\bm{x})} \le L.
    \end{equation}
\end{lemma}

\begin{proof}[of Lemma.~\ref{lemma:C21}]
    pass
\end{proof}

For Eq.~\ref{eq:C21}, it can be rewritten as
\begin{equation}
    -L \bm{I}_n \preceq \hess f(\bm{x}) \preceq L \bm{I}_n.
\end{equation}

The next statement is important for the geometric interpretation of functions in \(C_L^{1, 1}(\R^n)\)
\begin{lemma}\label{lemma:geometric-interpretation-C11L}
    Let \(f \in C_L^{1, 1}(\R^n)\). Then we have
    \begin{equation}
        \abs{f(\bm{y}) - f(\bm{x}) - \innerprod{\grad f(\bm{x})}{\bm{y} - \bm{x}}} \le \frac{L}{2} \norm{\bm{y} - \bm{x}}^2.
    \end{equation}
\end{lemma}

\begin{proof}[of Lemma.~\ref{lemma:geometric-interpretation-C11L}]
    pass
\end{proof}

The second main class of functions is type \(C^{2, 2}_M\), \emph{the class of twice differentiable functions with Lipschitz continuous Hessian}.
Recall that for \(f \in C^{2,2}_M(\R^n)\), we have
\[
    \norm{\hess f(\bm{x}) - \hess f(\bm{y})} \le M \norm{\bm{x} - \bm{y}}, \quad \forall \bm{x},~\bm{y} \in \R^n. 
\]

\begin{lemma}
    Let \(f \in C^{2,2}_M(\R^n)\). The for all \(\bm{x}, ~\bm{y} \in \R^n\) we have
    \begin{equation}\label{eq:C22M_1}
        \norm{\grad f(\bm{y}) - \grad f(\bm{x}) - \hess f(\bm{x})(\bm{y} -\bm{x})} \le \frac{M}{2} \norm{\bm{y} - \bm{x}}^2.
    \end{equation}
    \begin{equation}\label{eq:C22M_2}
        \abs{f(\bm{y}) - f(\bm{x}) - \innerprod{\grad f(\bm{x})}{\bm{y} - \bm{x}} - \frac{1}{2} \innerprod{\hess f(\bm{x})(\bm{y} -\bm{x})}{\bm{y} - \bm{x}}} \le \frac{M}{6} \norm{\bm{y} - \bm{x}}^3.
    \end{equation}
\end{lemma}

\begin{coro}
    Let \(f \in C^{2,2}_M(\R^n)\) and \(\bm{x},~\bm{y} \in \R^n\) with \(\norm{\bm{y} - \bm{x}} = r\). Then
    \[
        \hess f(\bm{x}) - Mr \bm{I}_n \preceq \hess f(\bm{y}) \preceq \hess f(\bm{x}) + Mr\bm{I}_n.  
    \]
\end{coro}